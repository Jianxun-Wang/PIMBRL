from copy import deepcopy
from random import randint
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import RLalgo as rla
from .dyna import dyna
from utility.utils import *
from utility.RL import *
import envs

class dyna(dyna):

    def __call__(self, epoch, real_policy_action_after,fake_policy_action_after,update_after,
                RL_batch_size,test_every,num_test_episodes,RL_update_iter, noiselist,
                fake_env_loss_criteria=0.008,env_train_start_size=4000,env_num_batch=20,
                data_train_max_iter=21,phy_train_max_iter=50,mixed_train=False,fake_len=3,
                usemodel = True,RL_loop=5,refresh_RL_buffer_interval=3):
        import time
        import math
        RL_trained_flag = False
        avgret = -float('inf')
        dataloss = 1e6#float('inf')
        count = 0
        o, ep_ret, ep_len = self.real_env.reset(), 0, 0
        returnlist = []
        sizelist = []
        returngoal = -55
        modelgoal = -120
        policygoal = -90
        rethis = torch.zeros([3],device='cpu')
        testcount = 0
        modellock = False
        k = 0
        precollect=True
        assert RL_loop > refresh_RL_buffer_interval
        if usemodel:
            if fake_len >= RL_loop:
                assert fake_len % refresh_RL_buffer_interval == 0
                block_len = fake_len//refresh_RL_buffer_interval

            else:
                block_len = 1
                assert RL_loop > fake_len
                refresh_RL_buffer_interval = fake_len


        # main loop
        for i in range(epoch):
            # interact with real env
            for j in range(1,self.env_batch_size+1):
                with torch.no_grad():
                    if i*j>real_policy_action_after: a = self.RL.ac.get_action(o, noiselist[i*j])
                    else: a = self.real_env.action_space.sample()
                    o, ep_ret, ep_len = self.interact_env(a, o, ep_ret, ep_len,self.real_env,
                        self.real_buffer,self.RL.max_ep_len,self.RL.ep_type,secondbuffer=self.RL.buffer)
            
            # train model with data loss
            if usemodel and self.real_buffer.size>env_train_start_size: 
                dataloss = self.trainenv(self.real_buffer,max_train_iter=data_train_max_iter,
                    phyloss=mixed_train,num_batch=env_num_batch,printflag=True)
            
            # update policy & value function with real data
            if self.real_buffer.size>update_after:
                for _ in range(RL_update_iter):
                    batch = self.real_buffer.sample_batch(RL_batch_size)
                    self.RL.update(data=batch)
                RL_trained_flag=True # to determine wether to test 
            
            # train model with equation loss or balance between data & equation training iterations
            if usemodel and self.RL.buffer.size>env_train_start_size:
                if self.phyloss_flag:
                    for _ in range(20):
                        self.trainenv(self.RL.buffer,phy_train_max_iter,
                            dataloss=False,phyloss=True,num_batch=env_num_batch)
                else:
                    for _ in range(20):
                        self.trainenv(self.RL.buffer,phy_train_max_iter,
                            dataloss=True,phyloss=False,num_batch=env_num_batch)

            # Fine-tune flag
            if avgret>returngoal: usemodel=False
            
            # train with data generated by model
      
            while usemodel:
                count +=1
                if dataloss<=fake_env_loss_criteria:
                # generate data with model
                    self.RL.env.eval()
                    # if avgret>=policygoal:policy = self.RL.ac.get_action
                    if self.RL.buffer.size>=fake_policy_action_after: policy = self.RL.ac.get_action
                    else: policy=None
                    for _ in range(block_len):
                        previoussize=self.RL.buffer.size
                        with torch.no_grad():
                            interact_fakeenv(self.RL.buffer,self.RL.buffer,
                                self.RL.env,self.RL.env.forward_size,noiselist[0],
                                policy,end=previoussize)

                    self.RL.env.train()
                    if self.phyloss_flag:
                        for _ in range(20):
                            self.trainenv(self.RL.buffer,phy_train_max_iter,
                                dataloss=False,phyloss=True,num_batch=env_num_batch)
                    else:
                        for _ in range(int(20/self.RL.buffer.size*self.real_buffer.size)):
                            self.trainenv(self.real_buffer,phy_train_max_iter,
                                dataloss=True,phyloss=False,num_batch=env_num_batch)           
                    
                if self.RL.buffer.size > update_after and dataloss<=fake_env_loss_criteria:
                    precollect = False
                    self.RL.env.eval()
                    # update policy & value with data from model
                    for _ in range(RL_update_iter):
                        batch = self.RL.buffer.sample_batch(RL_batch_size)
                        self.RL.update(data=batch)
                    self.RL.env.train()
                    # train model with equation loss or balance data loss iterations
                    
                
                    # test model on all data and remove fake data by resetting pointer
                    if count%refresh_RL_buffer_interval==0:
                        self.trainenv(self.RL.buffer,max_train_iter=1,phyloss=False,
                            num_batch=env_num_batch,printflag=True,trainflag=False)
                        self.RL.buffer.size,self.RL.buffer.ptr=self.real_buffer.size, self.real_buffer.ptr
                
                    # test agent
                    if count%test_every==0:
                        ret,_,_=test_RL(self.test_env,num_test_episodes,self.RL.max_ep_len, self.RL.ac,self.real_buffer.size,self.RL.gamma)
                        rethis[testcount%3]=ret
                        testcount+=1
                        avgret = ret#rethis.sum()/(rethis<0).sum()
                        print('\nModel: RL buffer size: {}\t Retrun: {} \t avgret:{}'.format(self.RL.buffer.size,ret,avgret))

                
                # exit control
                if (count>=RL_loop and (modellock or precollect)) or \
                   (avgret>modelgoal and not modellock) or count>=50 :
                    if not precollect: modellock = True
                    count=0
                    self.RL.buffer.size,self.RL.buffer.ptr=self.real_buffer.size, self.real_buffer.ptr
                    break
            
            # test & save
            if (RL_trained_flag and i%test_every==0)or i==0:
                ret,max,min=test_RL(self.test_env,num_test_episodes,self.RL.max_ep_len, self.RL.ac,i=self.real_buffer.size,parallel=True)
                rethis[(testcount)%3]=ret
                testcount+=1
                returnlist.append([ret,max,min])
                sizelist.append(self.real_buffer.size)
                # if RL_trained_flag: torch.save(self.RL.env,str(i)+' '+f"{dataloss:.9f}")
                returnhis = np.zeros([4,len(returnlist)])
                returnhis[1:] = np.array(returnlist).T
                returnhis[0]=np.array(sizelist)
                np.save('phy',returnhis)
                if usemodel:torch.save(self.RL.env,'model')
                torch.save(self.RL.ac,'agent')
                avgret = ret#rethis.sum()/(rethis<0).sum()
                print('\nReal buffer size: {}\t Retrun: {} \t avgret:{}'.format(self.real_buffer.size,ret,avgret))

    
    


if __name__=='__main__':
    import gym
    from src.NN import core,model
    from src.RLalgo import ddpg
    realenv = gym.make('CartPole-v0')
    fakeenv = model.fake_cartpole_env()
    RLinp = {"env":fakeenv,'actor_critic':core.MLPActorCritic,'ac_kwargs':dict(hidden_sizes=[256]*2,act_space_type='d'),'ep_type':'finite'}
    mb = dyna(ddpg.DDPG,RLinp,realenv,False)
    mb(10,400,1000,50,100,1,10,400,1000)
